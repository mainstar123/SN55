#!/usr/bin/env python3
"""
Fixed Model Trainer - Proper Normalization & Scaling for #1 Miner Performance
"""

import csv
import torch
import torch.nn as nn
import numpy as np
import json
from datetime import datetime, timezone
from sklearn.preprocessing import StandardScaler, RobustScaler

import sys
sys.path.append('.')
from advanced_attention_mechanisms import create_enhanced_attention_ensemble
from simple_data_fetch import SimpleCryptoDataFetcher


class FixedFeatureProcessor:
    """Process and normalize features properly"""

    def __init__(self):
        self.price_scaler = StandardScaler()
        self.volume_scaler = StandardScaler()
        self.feature_scaler = RobustScaler()  # More robust to outliers

    def normalize_features(self, data):
        """Normalize all features to similar scales"""

        # Extract raw data
        opens = np.array([row['open'] for row in data])
        highs = np.array([row['high'] for row in data])
        lows = np.array([row['low'] for row in data])
        closes = np.array([row['close'] for row in data])
        volumes = np.array([row['volume'] for row in data])

        # Normalize price data (fit on closes, transform all price features)
        self.price_scaler.fit(closes.reshape(-1, 1))
        opens_norm = self.price_scaler.transform(opens.reshape(-1, 1)).flatten()
        highs_norm = self.price_scaler.transform(highs.reshape(-1, 1)).flatten()
        lows_norm = self.price_scaler.transform(lows.reshape(-1, 1)).flatten()
        closes_norm = self.price_scaler.transform(closes.reshape(-1, 1)).flatten()

        # Normalize volume data
        self.volume_scaler.fit(volumes.reshape(-1, 1))
        volumes_norm = self.volume_scaler.transform(volumes.reshape(-1, 1)).flatten()

        # Basic normalized features
        features = [opens_norm, highs_norm, lows_norm, closes_norm, volumes_norm]
        feature_names = ['open_norm', 'high_norm', 'low_norm', 'close_norm', 'volume_norm']

        # Calculate normalized returns (much smaller scale now)
        returns = np.diff(closes_norm, prepend=closes_norm[0])
        features.append(returns)
        feature_names.append('returns_norm')

        # Add technical indicators on normalized data
        # Moving averages
        for window in [5, 10]:
            if len(closes_norm) >= window:
                ma = np.convolve(closes_norm, np.ones(window)/window, mode='valid')
                ma_padded = np.concatenate([np.full(window-1, closes_norm[0]), ma])
                features.append(ma_padded)
                feature_names.append(f'ma_{window}_norm')

        # RSI on normalized data
        def calculate_rsi(prices, period=14):
            deltas = np.diff(prices)
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)

            avg_gains = np.convolve(gains, np.ones(period)/period, mode='valid')
            avg_losses = np.convolve(losses, np.ones(period)/period, mode='valid')

            rs = np.concatenate([np.full(period, 1), avg_gains / (avg_losses + 1e-10)])
            rsi = 100 - (100 / (1 + rs))
            rsi_padded = np.concatenate([np.full(period, 50), rsi])
            # Normalize RSI to [-1, 1] range
            rsi_norm = (rsi_padded - 50) / 50
            return rsi_norm

        if len(closes_norm) >= 14:
            rsi = calculate_rsi(closes_norm)
            features.append(rsi)
            feature_names.append('rsi_norm')

        # Combine all features
        feature_matrix = np.column_stack(features)

        # Final scaling of entire feature matrix
        self.feature_scaler.fit(feature_matrix)
        feature_matrix_scaled = self.feature_scaler.transform(feature_matrix)

        return feature_matrix_scaled, feature_names

    def transform_targets(self, closes):
        """Transform targets to normalized price differences"""
        closes_norm = self.price_scaler.transform(closes.reshape(-1, 1)).flatten()

        targets = []
        for i in range(1, len(closes_norm)):
            # Normalized price difference (much more learnable)
            current = closes_norm[i-1]
            next_val = closes_norm[i]
            target = next_val - current  # Small, normalized differences
            targets.append(target)

        # Add first target as 0 (no previous data)
        targets.insert(0, 0.0)

        return np.array(targets, dtype=np.float32)

    def inverse_transform_predictions(self, predictions):
        """Convert normalized predictions back to price changes"""
        # Predictions are already in normalized price difference space
        # Convert back to actual price changes
        return predictions * self.price_scaler.scale_[0]


class FixedModelTrainer:
    """Train model with proper normalization and scaling"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.feature_processor = FixedFeatureProcessor()

    def prepare_fixed_data(self, csv_file='superior_data.csv', seq_len=30):
        """Prepare data with proper normalization"""

        print("üîß Preparing FIXED training data with proper normalization...")

        # Load raw data
        data = []
        with open(csv_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                data.append({
                    'open': float(row['open']),
                    'high': float(row['high']),
                    'low': float(row['low']),
                    'close': float(row['close']),
                    'volume': float(row['volume'])
                })

        # Normalize features
        feature_matrix, feature_names = self.feature_processor.normalize_features(data)
        print(f"‚úÖ Normalized {len(feature_names)} features")

        # Transform targets
        closes = np.array([row['close'] for row in data])
        targets = self.feature_processor.transform_targets(closes)

        # Create sequences with optimized length
        sequences = []
        sequence_targets = []

        for i in range(seq_len, len(feature_matrix)):
            seq = feature_matrix[i-seq_len:i]
            sequences.append(seq)

            # Target is the next normalized price difference
            sequence_targets.append(targets[i])

        X = np.array(sequences, dtype=np.float32)
        y = np.array(sequence_targets, dtype=np.float32)

        print(f"‚úÖ Created {len(X)} sequences of length {seq_len}")
        print(f"   Target range: {y.min():.6f} to {y.max():.6f} (properly normalized!)")

        return X, y, feature_names

    def train_fixed_model(self, X_train, y_train, input_size, epochs=50):
        """Train model with proper normalization"""

        print("üöÄ Training FIXED model with proper normalization...")

        model = create_enhanced_attention_ensemble(input_size=input_size)
        model.to(self.device)
        model.train()

        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Lower learning rate
        criterion = nn.MSELoss()
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.9)

        batch_size = 16
        train_dataset = torch.utils.data.TensorDataset(
            torch.from_numpy(X_train), torch.from_numpy(y_train)
        )
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        best_loss = float('inf')
        training_history = []

        for epoch in range(epochs):
            epoch_loss = 0
            n_batches = 0

            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)

                optimizer.zero_grad()

                outputs = model(batch_X)
                if isinstance(outputs, tuple):
                    predictions, _ = outputs
                else:
                    predictions = outputs

                if predictions.dim() > 1:
                    predictions = predictions.squeeze(-1)

                loss = criterion(predictions, batch_y)
                loss.backward()

                # Gradient clipping for stability
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                epoch_loss += loss.item()
                n_batches += 1

            avg_epoch_loss = epoch_loss / max(n_batches, 1)
            scheduler.step()
            training_history.append(avg_epoch_loss)

            if (epoch + 1) % 10 == 0:
                print(".6f")

            # Save best model
            if avg_epoch_loss < best_loss:
                best_loss = avg_epoch_loss
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'input_size': input_size,
                    'normalization': {
                        'price_scaler': {
                            'mean': self.feature_processor.price_scaler.mean_[0],
                            'scale': self.feature_processor.price_scaler.scale_[0]
                        },
                        'volume_scaler': {
                            'mean': self.feature_processor.volume_scaler.mean_[0],
                            'scale': self.feature_processor.volume_scaler.scale_[0]
                        },
                        'feature_scaler': 'RobustScaler fitted'
                    },
                    'training_loss': best_loss,
                    'epoch': epoch,
                    'timestamp': datetime.now(timezone.utc)
                }, 'fixed_superior_model.pth')
                print(f"üíæ Saved best fixed model (loss: {best_loss:.6f})")

        print("‚úÖ Fixed model training completed!")
        return model, best_loss, training_history

    def evaluate_fixed_model(self, model, X_test, y_test):
        """Evaluate the fixed model"""

        print("üìä Evaluating FIXED model performance...")

        model.eval()
        test_predictions = []
        test_actuals = []

        with torch.no_grad():
            for i in range(min(200, len(X_test))):  # Test on up to 200 samples
                x = torch.from_numpy(X_test[i:i+1]).to(self.device)
                actual = y_test[i]

                output = model(x)
                if isinstance(output, tuple):
                    pred, _ = output
                else:
                    pred = output

                pred_val = pred.cpu().numpy().flatten()[0]
                test_predictions.append(pred_val)
                test_actuals.append(actual)

        predictions = np.array(test_predictions)
        actuals = np.array(test_actuals)

        # Calculate metrics
        mape = np.mean(np.abs((predictions - actuals) / (np.abs(actuals) + 1e-6)))
        mse = np.mean((predictions - actuals) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(predictions - actuals))

        # Directional accuracy (key metric for trading)
        pred_direction = np.sign(predictions)
        actual_direction = np.sign(actuals)
        directional_acc = np.mean(pred_direction == actual_direction)

        results = {
            'mape': mape,
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'directional_accuracy': directional_acc,
            'total_predictions': len(predictions),
            'predictions_range': [float(predictions.min()), float(predictions.max())],
            'actuals_range': [float(actuals.min()), float(actuals.max())]
        }

        print(".4f")
        print(".4f")
        print(".4f")
        print(".1%")

        return results


def create_maintenance_system():
    """Create long-term maintenance system"""

    print("üîÑ Creating LONG-TERM MAINTENANCE SYSTEM...")

    maintenance_config = {
        'daily_updates': {
            'fresh_data_collection': 'Every 6 hours',
            'model_retraining': 'Daily at 02:00 UTC',
            'performance_evaluation': 'Every 4 hours',
            'competitor_monitoring': 'Continuous'
        },
        'retraining_triggers': {
            'performance_drop': '>5% MAPE increase',
            'competitor_surpass': 'When others exceed your performance',
            'market_regime_change': 'Major market condition shifts',
            'model_age': 'Every 7 days (prevent overfitting)'
        },
        'optimization_strategies': {
            'online_learning': 'Continuous small updates',
            'feature_expansion': 'Add market-specific indicators',
            'architecture_tuning': 'Hyperparameter optimization',
            'ensemble_methods': 'Multiple model combinations'
        },
        'monitoring_metrics': {
            'primary': ['mape', 'directional_accuracy', 'daily_tao_earnings'],
            'secondary': ['competitiveness_ratio', 'model_confidence', 'prediction_latency'],
            'alerts': {
                'critical': 'Performance drop >10%',
                'warning': 'Performance drop >5%',
                'info': 'New competitor emergence'
            }
        },
        'backup_systems': {
            'model_versions': 'Keep last 5 best performing models',
            'emergency_fallback': 'Simple baseline model',
            'data_backup': '7-day rolling data history'
        }
    }

    with open('maintenance_system.json', 'w') as f:
        json.dump(maintenance_config, f, indent=2)

    print("‚úÖ Maintenance system configuration saved")
    return maintenance_config


def run_fixed_training_pipeline():
    """Run the complete fixed training pipeline"""

    print("üéØ FIXED TRAINING PIPELINE FOR #1 MINER DOMINATION")
    print("=" * 60)

    # Initialize trainer
    trainer = FixedModelTrainer()

    # Step 1: Get comprehensive data
        print("\nüìä Step 1: Collecting Comprehensive Market Data...")
    fetcher = SimpleCryptoDataFetcher()
    fetcher.create_training_csv(['BTC', 'ETH', 'ADA', 'SOL', 'DOT', 'LINK'], 'fixed_training_data.csv')

    # Step 2: Prepare FIXED data with proper normalization
        print("\nüîß Step 2: Preparing FIXED Data (Proper Normalization)...")
    X, y, feature_names = trainer.prepare_fixed_data('fixed_training_data.csv', seq_len=30)

    # Split data
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    print(f"Training samples: {len(X_train)}")
    print(f"Test samples: {len(X_test)}")
    print(f"Features per sample: {X_train.shape[2]}")

    # Step 3: Train FIXED model
        print("\nüöÄ Step 3: Training FIXED Model (Proper Normalization)...")
    model, best_loss, history = trainer.train_fixed_model(X_train, y_train, X.shape[2], epochs=50)

    # Step 4: Evaluate FIXED model
        print("\nüìä Step 4: Evaluating FIXED Model Performance...")
    test_results = trainer.evaluate_fixed_model(model, X_test, y_test)

    # Step 5: Compare with Miner 221
        print("\nüèÜ Step 5: Competitiveness Analysis vs Miner 221...")

    # Miner 221 baseline
    miner221_reward = 0.239756  # TAO per prediction
    baseline_mape = 1.025  # Previous baseline

    improvement = (baseline_mape - test_results['mape']) / baseline_mape * 100
    estimated_reward = max(0, (1 - test_results['mape']) * 0.2)
    competitiveness = estimated_reward / miner221_reward

    print(".1f")
    print(".6f")
    print(".6f")
    print(".2f")

    # Performance assessment
    if test_results['mape'] < 0.4 and competitiveness > 0.9:
        status = "üöÄ ELITE DOMINATION - Ready to surpass Miner 221!"
        deployment_ready = True
    elif test_results['mape'] < 0.6 and competitiveness > 0.7:
        status = "‚úÖ EXCELLENT PERFORMANCE - Competitive with top miners!"
        deployment_ready = True
    elif test_results['mape'] < 0.8 and competitiveness > 0.5:
        status = "‚ö†Ô∏è GOOD PERFORMANCE - Above average, needs monitoring"
        deployment_ready = True
    else:
        status = "üîß NEEDS IMPROVEMENT - More optimization required"
        deployment_ready = False

    print(f"Status: {status}")
    print(f"Deployment Ready: {deployment_ready}")

    # Earnings projection
    daily_tao = estimated_reward * 24 * 6
    weekly_tao = daily_tao * 7
    monthly_tao = daily_tao * 30

        print("\nüí∞ EARNINGS PROJECTION:")
    print(".1f")
    print(".1f")
    print(".0f")
    # Success metrics check
    success_criteria = {
        'mape_target': test_results['mape'] < 0.6,
        'directional_acc_target': test_results['directional_accuracy'] > 0.52,
        'competitiveness_target': competitiveness > 0.7,
        'improvement_target': improvement > 50
    }

    success_score = sum(success_criteria.values())
    print("
üéØ SUCCESS METRICS:"    print(f"MAPE < 0.6: {success_criteria['mape_target']} ({test_results['mape']:.4f})")
    print(f"Directional Acc > 52%: {success_criteria['directional_acc_target']} ({test_results['directional_accuracy']:.1%})")
    print(f"Competitiveness > 0.7x: {success_criteria['competitiveness_target']} ({competitiveness:.2f}x)")
    print(f"Improvement > 50%: {success_criteria['improvement_target']} ({improvement:.1f}%)")
    print(f"Overall Success Score: {success_score}/4")

    # Save comprehensive results
    fixed_results = {
        'model_performance': test_results,
        'competitiveness_analysis': {
            'improvement_over_baseline': improvement,
            'estimated_tao_per_prediction': estimated_reward,
            'competitiveness_vs_miner221': competitiveness,
            'miner221_baseline': miner221_reward
        },
        'training_info': {
            'total_samples': len(X_train),
            'test_samples': len(X_test),
            'features_used': len(feature_names),
            'sequence_length': 30,
            'final_training_loss': best_loss,
            'epochs_trained': 50
        },
        'earnings_projection': {
            'daily_tao': daily_tao,
            'weekly_tao': weekly_tao,
            'monthly_tao': monthly_tao
        },
        'assessment': {
            'status': status,
            'deployment_ready': deployment_ready,
            'success_score': success_score,
            'model_saved': 'fixed_superior_model.pth'
        },
        'normalization_info': {
            'price_scaler_mean': trainer.feature_processor.price_scaler.mean_[0],
            'price_scaler_scale': trainer.feature_processor.price_scaler.scale_[0],
            'volume_scaler_mean': trainer.feature_processor.volume_scaler.mean_[0],
            'volume_scaler_scale': trainer.feature_processor.volume_scaler.scale_[0]
        },
        'timestamp': datetime.now(timezone.utc).isoformat()
    }

    with open('fixed_training_results.json', 'w') as f:
        json.dump(fixed_results, f, indent=2, default=str)

    print("
‚úÖ FIXED TRAINING COMPLETED!"    print("Results saved to: fixed_training_results.json")
    print("Model saved to: fixed_superior_model.pth")

    # Create maintenance system
    maintenance = create_maintenance_system()

    print("
üîÑ MAINTENANCE SYSTEM ACTIVATED!"    print("Configuration saved to: maintenance_system.json")

    return fixed_results, maintenance


if __name__ == "__main__":
    results, maintenance = run_fixed_training_pipeline()

    print("
üéä PIPELINE COMPLETE!"    print(f"Success Score: {results['assessment']['success_score']}/4")

    if results['assessment']['deployment_ready']:
        print("üöÄ READY FOR DEPLOYMENT!")
        print("Run: python3 start_domination_miner.py --model fixed_superior_model.pth --deploy")
    else:
        print("üîß MORE OPTIMIZATION NEEDED")
        print("Review results and adjust training parameters")
