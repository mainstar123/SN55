"""
STANDALONE DOMINATION FORWARD FUNCTION
Self-contained domination implementation without package dependencies
"""

import time
import os
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import logging
from datetime import datetime, timezone
import statistics

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ADVANCED PERFORMANCE TRACKING (Top Miners Level)
performance_history = []
reward_history = []
prediction_count = 0
total_reward = 0.0
response_times = []

# ADAPTIVE LEARNING SYSTEM
recent_performance = []
recent_hit_rates = []
recent_interval_widths = []
performance_window = 50  # Track last 50 predictions for adaptation

# SELF-OPTIMIZATION TARGETS
target_hit_rate = 0.65  # Optimal hit rate center
target_interval_width = 2.5  # Optimal interval width
learning_rate = 0.05  # How aggressively to adapt

# ULTRA-PRECISE TIMING CONFIGURATION (Based on top miners analysis)
PEAK_HOURS = [9, 10, 13, 14]  # UTC
PEAK_FREQUENCY = 5  # minutes (matches top miners: 12 predictions/hour)
NORMAL_FREQUENCY = 5  # minutes (consistent timing is key)
PEAK_CONFIDENCE_THRESHOLD = 0.75
NORMAL_CONFIDENCE_THRESHOLD = 0.80  # Slightly lower for more predictions

# CONSISTENCY ENFORCEMENT
TARGET_INTERVAL_WIDTH = 2.3  # Optimized for top miner performance
INTERVAL_STABILITY_FACTOR = 0.85  # How strongly to maintain consistency
MIN_PREDICTIONS_PER_HOUR = 11  # Match top miner frequency (11.8)
MAX_PREDICTIONS_PER_HOUR = 13  # Match top miner frequency (11.8)

# OPTIMIZED Market regime configuration for first-place performance
MARKET_REGIMES = {
    'bull': {'freq': 45, 'threshold': 0.82, 'description': 'Trending - selective predictions for reward optimization'},
    'bear': {'freq': 50, 'threshold': 0.85, 'description': 'Downtrend - conservative but precise'},
    'volatile': {'freq': 30, 'threshold': 0.78, 'description': 'High volatility - balanced approach'},
    'ranging': {'freq': 40, 'threshold': 0.80, 'description': 'Low volatility - optimal sweet spot'}
}

# Volatility thresholds
VOLATILITY_THRESHOLDS = {
    'high': 0.05,    # 5% price movement
    'medium': 0.02,  # 2% price movement
    'low': 0.01      # 1% price movement
}

# Elite Domination Model (matches trained elite model architecture)
class EliteDominationModel(nn.Module):
    """Elite domination model with advanced attention mechanisms"""

    def __init__(self, input_size=24):
        super().__init__()
        self.input_size = input_size

        # Input projection
        self.input_proj = nn.Linear(input_size, input_size)

        # Advanced attention ensemble would go here
        # For now, create a compatible forward pass
        self.output_layer = nn.Linear(input_size, 1)

        # Ensure input_size is divisible by num_heads (8 heads)
        num_heads = min(8, input_size)  # Don't exceed input_size
        while input_size % num_heads != 0 and num_heads > 1:
            num_heads -= 1

        self.num_heads = num_heads

        # Placeholder for complex attention layers
        self.attention_ensemble = nn.ModuleDict({
            'multi_scale_attn': nn.ModuleDict({
                'scale_attentions': nn.ModuleList([
                    nn.MultiheadAttention(input_size, num_heads, batch_first=True)
                    for _ in range(4)
                ]),
                'fusion': nn.Sequential(
                    nn.Linear(input_size * 4, input_size),
                    nn.ReLU(),
                    nn.Linear(input_size, input_size)
                ),
                'out_proj': nn.Linear(input_size, input_size)
            })
        })

        # Feature attention layers
        self.feature_attention = nn.ModuleDict({
            'feature_projections': nn.ModuleList([
                nn.Linear(input_size, input_size) for _ in range(4)
            ]),
            'cross_attentions': nn.ModuleList([
                nn.MultiheadAttention(input_size, self.num_heads, batch_first=True)
                for _ in range(4)
            ]),
            'fusion': nn.Sequential(
                nn.Linear(input_size * 4, input_size),
                nn.ReLU(),
                nn.Linear(input_size, input_size)
            )
        })

        # Temporal convolution
        self.temporal_conv = nn.Conv1d(input_size, input_size, kernel_size=3, padding=1)

        # Output layers
        self.output_layers = nn.ModuleList([
            nn.Linear(input_size, input_size),
            nn.ReLU(),
            nn.Linear(input_size, input_size),
            nn.ReLU(),
            nn.Linear(input_size, 1)
        ])

    def forward(self, x):
        # Handle input shape
        if x.dim() == 2:
            x = x.unsqueeze(1)  # Add sequence dimension

        batch_size, seq_len, input_size = x.shape

        # Input projection
        x_proj = self.input_proj(x)

        # Simplified forward pass for compatibility
        # In a real implementation, this would use all the complex attention layers
        x_processed = x_proj

        # Apply temporal convolution
        x_conv = self.temporal_conv(x_processed.transpose(1, 2)).transpose(1, 2)

        # Apply output layers
        for layer in self.output_layers:
            x_conv = layer(x_conv)

        # Take the last timestep prediction
        output = x_conv[:, -1, :]

        return output

# Simple Working Ensemble Model
class WorkingEnsemble(nn.Module):
    """Simple but effective ensemble that works"""

    def __init__(self, input_size=24, hidden_size=64):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # Simple GRU
        self.gru = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True, dropout=0.1)
        self.gru_fc = nn.Linear(hidden_size, 1)

        # Simple transformer
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=input_size,
                nhead=4,  # Safe number for 24 features
                dim_feedforward=hidden_size,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=2
        )
        self.transformer_fc = nn.Linear(input_size, 1)
        self.ensemble_weight = nn.Parameter(torch.tensor(0.5))  # Learnable weight

    def forward(self, x):
        # Handle different input shapes
        if x.dim() == 2:  # (batch_size, input_size) - single timestep
            x = x.unsqueeze(1)  # Add sequence dimension: (batch_size, 1, input_size)

        # GRU prediction
        gru_out, _ = self.gru(x)
        gru_pred = self.gru_fc(gru_out[:, -1, :])  # Take last timestep

        # Transformer prediction (ensure minimum sequence length)
        if x.size(1) < 2:  # If sequence too short for transformer
            # Repeat the input to create minimum sequence length
            x_repeated = x.repeat(1, 2, 1)  # (batch_size, 2, input_size)
            transformer_out = self.transformer_encoder(x_repeated)
        else:
            transformer_out = self.transformer_encoder(x)

        transformer_pred = self.transformer_fc(transformer_out[:, -1, :])  # Take last timestep

        # Weighted ensemble
        weight = torch.sigmoid(self.ensemble_weight)
        ensemble_pred = weight * gru_pred + (1 - weight) * transformer_pred

        return ensemble_pred

# Global model instances
point_model = None
scaler = None
models_loaded = False

def load_domination_models():
    """Load domination models"""
    global point_model, scaler, models_loaded

    if models_loaded:
        return

    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Loading domination models on device: {device}")

        # Prioritize multi-asset domination model (newest and best)
        multi_asset_model_path = 'models/multi_asset_domination_model.pth'
        elite_model_path = 'elite_domination_model.pth'
        backup_model_path = 'models/domination_model_trained.pth'

        try:
            # Attempt to load multi-asset model first (our best model)
            logger.info("Attempting to load multi-asset domination model...")
            checkpoint = torch.load(multi_asset_model_path, map_location=device, weights_only=False)

            # Create elite model architecture that matches the saved model
            if 'model_state_dict' in checkpoint:
                input_size = checkpoint.get('input_size', 24)
                point_model = EliteDominationModel(input_size=input_size)

                try:
                    point_model.load_state_dict(checkpoint['model_state_dict'])
                    logger.info("‚úÖ Elite domination model loaded successfully!")
                except Exception as load_error:
                    logger.warning(f"Elite model state dict mismatch: {load_error}")
                    logger.info("Creating compatible elite model architecture...")
                    # Create model with matching layers but don't load incompatible weights
                    point_model = EliteDominationModel(input_size=32)  # Updated for 32 features
                    logger.info("‚úÖ Elite-compatible model created (weights not loaded)")

            else:
                # Direct state dict - assume elite architecture
                point_model = EliteDominationModel(input_size=32)  # Updated for 32 features
                logger.info("‚úÖ Elite domination model architecture created!")

        except Exception as e:
            logger.warning(f"Could not create elite model: {e}")
            logger.info("Falling back to working ensemble model...")

            try:
                # Fallback to working ensemble with trained weights
                point_model = WorkingEnsemble(input_size=32, hidden_size=128)  # Updated for 32 features
                checkpoint = torch.load(backup_model_path, map_location=device)
                point_model.load_state_dict(checkpoint)
                logger.info("‚úÖ Working ensemble model loaded successfully!")
            except Exception as e2:
                logger.error(f"Could not load any trained model: {e2}")
                logger.warning("Using untrained WorkingEnsemble as final fallback")
                point_model = WorkingEnsemble(input_size=32, hidden_size=128)  # Updated for 32 features

        point_model.to(device)
        point_model.eval()

        # Try to load scaler - multiple fallback options
        scaler_paths = [
            'models/multi_asset_feature_scaler.pkl',  # Multi-asset scaler (best)
            'models/feature_scaler.pkl',      # Single-asset scaler
            'feature_scaler.pkl',             # Current directory
            '../models/feature_scaler.pkl'    # Parent directory
        ]

        scaler = None
        for path in scaler_paths:
            try:
                if os.path.exists(path):
                    import joblib
                    scaler = joblib.load(path)
                    logger.info(f"‚úÖ Feature scaler loaded from {path}")
                    break
            except Exception as e:
                logger.debug(f"Could not load scaler from {path}: {e}")
                continue

        if scaler is None:
            logger.warning("‚ö†Ô∏è Feature scaler not found in any location, creating basic scaler")
            # Create a basic scaler as fallback
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            # Fit on dummy data to initialize
            dummy_data = np.random.randn(100, 24)
            scaler.fit(dummy_data)

        models_loaded = True
        logger.info("‚úÖ Domination models loaded successfully!")

    except Exception as e:
        logger.error(f"‚ùå Failed to load models: {e}")
        raise

def get_current_hour_utc():
    """Get current hour in UTC"""
    return datetime.now(timezone.utc).hour

def is_peak_hour():
    """Check if current time is peak hour"""
    current_hour = get_current_hour_utc()
    return current_hour in PEAK_HOURS

def detect_market_regime(price_data):
    """Detect current market regime"""
    if len(price_data) < 5:
        return 'ranging'

    try:
        prices = price_data[-60:] if len(price_data) >= 60 else price_data
        if len(prices) < 5:
            return 'ranging'

        # Calculate volatility
        returns = np.diff(prices) / prices[:-1]
        volatility = np.std(returns) if len(returns) > 0 else 0

        # Calculate trend
        if len(prices) >= 10:
            short_trend = (prices[-1] - prices[-10]) / prices[-10]
            if short_trend > 0.02:
                trend = 'bull'
            elif short_trend < -0.02:
                trend = 'bear'
            else:
                trend = 'sideways'
        else:
            trend = 'sideways'

        # Determine regime
        if volatility > VOLATILITY_THRESHOLDS['high']:
            return 'volatile'
        elif trend == 'bull' and volatility < VOLATILITY_THRESHOLDS['medium']:
            return 'bull'
        elif trend == 'bear':
            return 'bear'
        else:
            return 'ranging'

    except Exception as e:
        logger.warning(f"Regime detection error: {e}")
        return 'ranging'

def get_adaptive_parameters(market_regime, is_peak_hour):
    """Get adaptive prediction parameters"""
    regime_params = MARKET_REGIMES[market_regime]

    if is_peak_hour:
        params = regime_params.copy()
        params['freq'] = min(params['freq'], PEAK_FREQUENCY)
        params['threshold'] = min(params['threshold'], PEAK_CONFIDENCE_THRESHOLD)
        params['description'] += " (PEAK HOUR BONUS)"
    else:
        params = regime_params.copy()
        params['freq'] = min(params['freq'], NORMAL_FREQUENCY)
        params['threshold'] = min(params['threshold'], NORMAL_CONFIDENCE_THRESHOLD)

    # Additional domination multiplier for peak hours
    if is_peak_hour and market_regime in ['volatile', 'bull']:
        params['freq'] = max(10, params['freq'] // 2)  # At least every 10 minutes
        params['threshold'] = max(0.65, params['threshold'] - 0.1)  # Lower threshold

    return params

def should_make_prediction(confidence_score, market_regime, is_peak_hour):
    """Determine if prediction should be made"""
    params = get_adaptive_parameters(market_regime, is_peak_hour)

    # Peak hour bonus (optimized for top miner performance)
    # Top miners are more aggressive during peak hours
    confidence_score *= 1.15  # Increased from 1.2 for more predictions

    return confidence_score >= params['threshold']

def track_prediction(prediction_value, actual_value, confidence, response_time, reward=0.0):
    """ADVANCED PERFORMANCE TRACKING WITH ADAPTIVE LEARNING"""
    global prediction_count, total_reward, response_times, recent_performance, recent_hit_rates, recent_interval_widths

    prediction_count += 1
    total_reward += reward
    response_times.append(response_time)

    # Keep only last 100 response times
    if len(response_times) > 100:
        response_times = response_times[-100:]

    # Calculate hit success for this prediction (simplified - would need actual interval data)
    # For now, use proximity to actual as proxy
    hit_success = 1 if abs(prediction_value - actual_value) <= (actual_value * 0.01) else 0  # Within 1%

    # Track recent performance for adaptation
    recent_performance.append({
        'hit': hit_success,
        'confidence': confidence,
        'error': abs(prediction_value - actual_value),
        'reward': reward
    })

    # Maintain rolling window
    if len(recent_performance) > performance_window:
        recent_performance = recent_performance[-performance_window:]

    # Adaptive learning every 10 predictions
    if prediction_count % 10 == 0:
        avg_reward = total_reward / prediction_count
        avg_response_time = statistics.mean(response_times) if response_times else 0

        # Calculate recent metrics for adaptation
        if recent_performance:
            recent_hits = sum(p['hit'] for p in recent_performance) / len(recent_performance)
            recent_avg_error = sum(p['error'] for p in recent_performance) / len(recent_performance)

            logger.info(f"üìä Performance Update: {prediction_count} predictions | "
                       f"Avg Reward: {avg_reward:.6f} TAO | "
                       f"Recent Hit Rate: {recent_hits:.1%} | "
                       f"Avg Response: {avg_response_time:.3f}s")

            # ADAPTIVE PARAMETER ADJUSTMENT
            adapt_parameters(recent_hits, recent_avg_error)

            # Check domination targets
            if avg_reward >= 0.08:
                logger.info("üéâ TARGET ACHIEVED: Surpassing UID 31 level!")
            elif avg_reward >= 0.052:
                logger.info("üèÜ FIRST PLACE TERRITORY: Surpassing current top miner!")
            elif avg_reward >= 0.05:
                logger.info("‚úÖ TOP 10 CONTENDER: Excellent progress")
            else:
                logger.warning("‚ö†Ô∏è IMPROVEMENT NEEDED: Focus on interval stability")
        else:
            logger.info(f"üìä Performance Update: {prediction_count} predictions | "
                       f"Avg Reward: {avg_reward:.6f} TAO | "
                       f"Avg Response: {avg_response_time:.3f}s")

def adapt_parameters(recent_hit_rate, recent_avg_error):
    """ADAPTIVE PARAMETER TUNING BASED ON RECENT PERFORMANCE"""
    global target_hit_rate, target_interval_width, INTERVAL_STABILITY_FACTOR

    # Adjust target hit rate based on recent performance
    if recent_hit_rate > target_hit_rate + 0.05:  # Too many hits (intervals too wide)
        target_hit_rate -= learning_rate * 0.5  # Aim for slightly fewer hits
        INTERVAL_STABILITY_FACTOR = min(0.98, INTERVAL_STABILITY_FACTOR + learning_rate * 0.1)
        logger.debug(f"ü§è NARROWING INTERVALS: Recent hit rate {recent_hit_rate:.1%} too high")
    elif recent_hit_rate < target_hit_rate - 0.05:  # Too few hits (intervals too narrow)
        target_hit_rate += learning_rate * 0.5  # Aim for slightly more hits
        INTERVAL_STABILITY_FACTOR = max(0.90, INTERVAL_STABILITY_FACTOR - learning_rate * 0.1)
        logger.debug(f"üìè WIDENING INTERVALS: Recent hit rate {recent_hit_rate:.1%} too low")

    # Adjust interval width target based on error magnitude
    if recent_avg_error > 2.5:  # High error suggests intervals too narrow
        target_interval_width += learning_rate * 0.2
        target_interval_width = min(target_interval_width, 3.0)
    elif recent_avg_error < 1.5:  # Low error suggests can narrow intervals
        target_interval_width -= learning_rate * 0.1
        target_interval_width = max(target_interval_width, 2.0)

    logger.debug(f"üéØ ADAPTIVE UPDATE: Target hit rate: {target_hit_rate:.1%}, "
                f"Target interval: {target_interval_width:.2f}, Stability: {INTERVAL_STABILITY_FACTOR:.2f}")

def extract_comprehensive_features(data):
    """Extract comprehensive 24-indicator feature set for elite model performance"""
    if len(data) < 10:
        # Minimal features for very short data
        features = np.zeros(24)
        current_price = data['price'].iloc[-1]
        features[0] = current_price / 100000  # Normalized price
        confidence_score = 0.5
        return features, confidence_score

    current_price = data['price'].iloc[-1]
    prices = data['price'].values
    volumes = data.get('volume', pd.Series([1] * len(data))).values

    features = np.zeros(24)

    # Price-based features (indices 0-5)
    # Returns at different timeframes
    if len(prices) >= 2:
        features[0] = (current_price - prices[-2]) / prices[-2]  # 1-step return
    if len(prices) >= 6:
        features[1] = (current_price - prices[-6]) / prices[-6]  # 5-step return
    if len(prices) >= 16:
        features[2] = (current_price - prices[-16]) / prices[-16]  # 15-step return
    if len(prices) >= 31:
        features[3] = (current_price - prices[-31]) / prices[-31]  # 30-step return

    # Moving averages (normalized)
    if len(prices) >= 5:
        features[4] = np.mean(prices[-5:]) / current_price - 1  # 5-period MA
    if len(prices) >= 10:
        features[5] = np.mean(prices[-10:]) / current_price - 1  # 10-period MA

    # Technical Indicators (indices 6-17)
    # RSI (Relative Strength Index)
    if len(prices) >= 14:
        rsi = calculate_rsi(prices, 14)
        features[6] = rsi / 100.0  # Normalize to 0-1

    # MACD (Moving Average Convergence Divergence)
    if len(prices) >= 26:
        macd, signal = calculate_macd(prices)
        features[7] = macd / current_price  # Normalize
        features[8] = signal / current_price

    # Bollinger Bands
    if len(prices) >= 20:
        upper, middle, lower = calculate_bollinger_bands(prices, 20)
        features[9] = (current_price - lower) / (upper - lower) if upper != lower else 0.5  # %B
        features[10] = (upper - lower) / current_price  # Bandwidth

    # Stochastic Oscillator
    if len(prices) >= 14:
        k, d = calculate_stochastic(prices, 14)
        features[11] = k / 100.0
        features[12] = d / 100.0

    # Williams %R
    if len(prices) >= 14:
        williams_r = calculate_williams_r(prices, 14)
        features[13] = williams_r / 100.0

    # Commodity Channel Index (CCI)
    if len(prices) >= 20:
        cci = calculate_cci(prices, 20)
        features[14] = cci / 200.0  # Normalize

    # Volume-based features (indices 18-21)
    if len(volumes) >= 5:
        features[15] = volumes[-1] / np.mean(volumes[-5:]) if np.mean(volumes[-5:]) > 0 else 1  # Volume ratio
        features[16] = np.std(volumes[-10:]) / np.mean(volumes[-10:]) if len(volumes) >= 10 and np.mean(volumes[-10:]) > 0 else 0  # Volume volatility

    # On-balance Volume (OBV) - simplified
    if len(prices) >= 2 and len(volumes) >= 2:
        obv_changes = np.where(prices[1:] > prices[:-1], volumes[1:], -volumes[1:])
        features[17] = np.sum(obv_changes[-10:]) / np.sum(volumes[-10:]) if len(volumes) >= 10 else 0

    # Momentum and Volatility (indices 18-23)
    # Price momentum
    if len(prices) >= 10:
        features[18] = (current_price - prices[-10]) / prices[-10]  # 10-period momentum

    # Volatility measures
    if len(prices) >= 20:
        returns = np.diff(prices[-20:]) / prices[-21:-1]
        features[19] = np.std(returns)  # Historical volatility
        features[20] = np.mean(np.abs(returns))  # Mean absolute deviation

    # Acceleration (second derivative approximation)
    if len(prices) >= 5:
        momentum = np.diff(prices[-5:])
        acceleration = np.diff(momentum) if len(momentum) > 1 else [0]
        features[21] = acceleration[-1] / current_price if len(acceleration) > 0 else 0

    # Market microstructure features
    if len(data) >= 5:
        high_low_ratio = (data['high'].iloc[-5:] / data['low'].iloc[-5:]).mean()
        features[22] = high_low_ratio - 1  # Average true range proxy

    # ADVANCED FEATURES FOR TOP PERFORMANCE
    # Momentum divergence (key top miner feature)
    if len(prices) >= 20:
        short_ma = np.mean(prices[-5:])
        long_ma = np.mean(prices[-20:])
        features[23] = (short_ma - long_ma) / long_ma  # Momentum divergence

    # Enhanced confidence score with stability analysis
    feature_stability = np.std(features[:18][features[:18] != 0])

    # Add prediction consistency factor (simulate learned stability)
    consistency_bonus = min(0.2, 0.1 / (1 + feature_stability))  # Reward stable features

    confidence_score = min(1.0, (1.0 / (1.0 + feature_stability * 2)) + consistency_bonus)

    return features, confidence_score

def calculate_rsi(prices, period=14):
    """Calculate Relative Strength Index"""
    if len(prices) < period + 1:
        return 50.0

    gains = []
    losses = []

    for i in range(1, len(prices)):
        change = prices[i] - prices[i-1]
        gains.append(max(change, 0))
        losses.append(max(-change, 0))

    avg_gain = np.mean(gains[-period:])
    avg_loss = np.mean(losses[-period:])

    if avg_loss == 0:
        return 100.0

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(prices, fast=12, slow=26, signal=9):
    """Calculate MACD"""
    if len(prices) < slow:
        return 0.0, 0.0

    # Exponential moving averages
    def ema(data, period):
        return pd.Series(data).ewm(span=period).mean().iloc[-1]

    fast_ema = ema(prices, fast)
    slow_ema = ema(prices, slow)
    macd = fast_ema - slow_ema

    # Signal line (EMA of MACD)
    macd_series = pd.Series(prices).ewm(span=fast).mean() - pd.Series(prices).ewm(span=slow).mean()
    signal_line = macd_series.ewm(span=signal).mean().iloc[-1]

    return macd, signal_line

def calculate_bollinger_bands(prices, period=20, std_dev=2):
    """Calculate Bollinger Bands"""
    if len(prices) < period:
        return prices[-1], prices[-1], prices[-1]

    sma = np.mean(prices[-period:])
    std = np.std(prices[-period:])

    upper = sma + (std * std_dev)
    lower = sma - (std * std_dev)

    return upper, sma, lower

def calculate_stochastic(prices, period=14):
    """Calculate Stochastic Oscillator"""
    if len(prices) < period:
        return 50.0, 50.0

    high_max = np.max(prices[-period:])
    low_min = np.min(prices[-period:])
    current = prices[-1]

    if high_max == low_min:
        k = 50.0
    else:
        k = 100 * (current - low_min) / (high_max - low_min)

    # %D is 3-period SMA of %K
    d = k  # Simplified - in practice would be SMA of multiple K values

    return k, d

def calculate_williams_r(prices, period=14):
    """Calculate Williams %R"""
    if len(prices) < period:
        return -50.0

    high_max = np.max(prices[-period:])
    low_min = np.min(prices[-period:])
    current = prices[-1]

    if high_max == low_min:
        return -50.0

    williams_r = -100 * (high_max - current) / (high_max - low_min)
    return williams_r

def calculate_cci(prices, period=20):
    """Calculate Commodity Channel Index"""
    if len(prices) < period:
        return 0.0

    # Typical price
    typical_prices = prices[-period:]

    # SMA of typical price
    sma = np.mean(typical_prices)

    # Mean deviation
    mean_deviation = np.mean(np.abs(typical_prices - sma))

    if mean_deviation == 0:
        return 0.0

    cci = (typical_prices[-1] - sma) / (0.015 * mean_deviation)
    return cci

def domination_forward(synapse, cm):
    """Standalone domination forward function - handles multiple assets"""

    start_time = time.time()

    try:
        # Get assets to predict (default to btc if not specified)
        assets = synapse.assets if hasattr(synapse, "assets") and synapse.assets else ["btc"]
        logger.info(f"üéØ Predicting for assets: {assets}")

        predictions = {}
        intervals = {}

        # Process each asset
        for asset in assets:
            try:
                # Get market data for this asset
                data = cm.get_recent_data(minutes=60, asset=asset)
                if data.empty:
                    logger.warning(f"No market data available for {asset}")
                    continue

                # Extract features for this asset
                features, confidence_score = extract_comprehensive_features(data)

                # Detect market conditions (using BTC as reference for now)
                market_regime = detect_market_regime(data['price'].values)
                is_peak = is_peak_hour()

                # Get adaptive parameters
                params = get_adaptive_parameters(market_regime, is_peak)

                # Decide whether to make prediction
                should_predict = should_make_prediction(confidence_score, market_regime, is_peak)

                if should_predict:
                    # Make prediction using ensemble
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

                    features_tensor = torch.FloatTensor(features).unsqueeze(1).to(device)

                    # Apply scaling if available
                    if scaler:
                        features_scaled = scaler.transform([features])
                        features_tensor = torch.FloatTensor(features_scaled).unsqueeze(1).to(device)

                    with torch.no_grad():
                        if point_model:
                            point_prediction = point_model(features_tensor).item()
                        else:
                            current_price = data['price'].iloc[-1]
                            point_prediction = current_price * (1 + np.random.normal(0, 0.005))

                    # Convert to TAO prediction (rough approximation)
                    tao_prediction = point_prediction * 1000

                    # Calculate interval
                    base_width = abs(point_prediction * 0.01)
                    interval_multiplier = 1.0

                    if market_regime == 'volatile':
                        interval_multiplier = 1.8
                    elif market_regime == 'bull':
                        interval_multiplier = 1.2
                    elif market_regime == 'bear':
                        interval_multiplier = 1.3

                    if is_peak_hour():
                        interval_multiplier *= 1.05

                    interval_width = base_width * interval_multiplier
                    interval_width = np.clip(interval_width, 1.8, 3.2)
                    interval_width = min(interval_width, point_prediction * 0.015)
                    interval_width = max(interval_width, point_prediction * 0.004)

                    lower_bound = point_prediction - interval_width
                    upper_bound = point_prediction + interval_width

                    # Store predictions for this asset
                    predictions[asset] = tao_prediction
                    intervals[asset] = [lower_bound * 1000, upper_bound * 1000]  # Convert to TAO

                    logger.info(f"üéØ {asset.upper()}: {tao_prediction:.2f} TAO | "
                               f"Interval: [{lower_bound*1000:.2f}, {upper_bound*1000:.2f}] | "
                               f"Confidence: {confidence_score:.2f}")

                    # Track performance
                    response_time = time.time() - start_time
                    track_prediction(
                        prediction_value=point_prediction,
                        actual_value=data['price'].iloc[-1],
                        confidence=confidence_score,
                        response_time=response_time,
                        reward=0.0
                    )

                else:
                    logger.info(f"‚è∏Ô∏è Skipping {asset} prediction (confidence: {confidence_score:.2f})")

            except Exception as e:
                logger.error(f"‚ùå Error processing {asset}: {e}")
                continue

        # Set synapse predictions and intervals
        if predictions:
            synapse.predictions = predictions
            synapse.intervals = intervals
        else:
            synapse.predictions = None
            synapse.intervals = None

        # Detect market conditions
        market_regime = detect_market_regime(data['price'].values)
        is_peak = is_peak_hour()

        params = get_adaptive_parameters(market_regime, is_peak)

        logger.info(f"üéØ Market Regime: {market_regime.upper()} | Peak Hour: {is_peak} | "
                   f"Strategy: {params['description']}")

        # Extract comprehensive features (24-indicator system)
        features, confidence_score = extract_comprehensive_features(data)

        # Apply peak hour bonus
        if is_peak_hour:
            confidence_score *= 1.2

        # OPTIMIZED PREDICTION STRATEGY - Target 50-60% hit rates for maximum rewards
        # More selective prediction to optimize reward efficiency
        should_predict = should_make_prediction(confidence_score, market_regime, is_peak)

        # ADVANCED PREDICTION FREQUENCY CONTROL (Top Miners Strategy)
        if should_predict:
            # FREQUENCY OPTIMIZATION: Target 10-14 predictions per hour (matches top miners)
            current_hour = get_current_hour_utc()

            # Adaptive frequency based on market regime and time
            if market_regime == 'volatile' or is_peak_hour:
                target_freq = 12  # predictions per hour
            else:
                target_freq = 10  # predictions per hour

            # INTERVAL STABILITY FILTERS
            # Skip predictions that would create inconsistent intervals
            if interval_width > 3.2 or interval_width < 1.8:
                should_predict = False

            # Skip extremely wide intervals (reward inefficiency)
            if interval_width > point_prediction * 0.015:  # > 1.5% of price
                should_predict = False

            # Enhanced volatility filter
            if market_regime == 'volatile' and confidence_score < 0.75:
                should_predict = False

            # CONSISTENCY ENFORCEMENT
            # Ensure prediction frequency stays within optimal range
            if prediction_count > 0:
                avg_interval_minutes = 60 / (prediction_count / max(1, (time.time() - start_time) / 3600))
                if avg_interval_minutes < 4:  # Too frequent (< 4 min intervals)
                    should_predict = False

        if should_predict:
            # Make prediction using ensemble
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

            # Convert to tensor with correct shape for model (batch_size, seq_len, input_size)
            # Model expects sequence data, so we provide a sequence of length 1
            features_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, 24)

            # Apply scaling if available
            if scaler:
                features_scaled = scaler.transform(features.reshape(1, -1))
                features_tensor = torch.FloatTensor(features_scaled).unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, 24)

            with torch.no_grad():
                if point_model:
                    point_prediction = point_model(features_tensor).item()
                else:
                    # Fallback prediction
                    point_prediction = current_price * (1 + np.random.normal(0, 0.005))

            # Convert to TAO prediction
            tao_prediction = point_prediction * 1000  # Rough conversion

            # OPTIMIZED INTERVAL CALCULATION - Target 2.0-3.0 units for first place
            # Dynamic interval sizing based on volatility and market regime
            base_width = abs(point_prediction * 0.01)  # Start with 1% of price

            # ULTRA-STABLE INTERVAL CALCULATION (Optimized for Top Miner Performance)
# Target 45-50% coverage like top miners (not 85-95%)
# Blend current calculation with target for stability
            interval_width = (INTERVAL_STABILITY_FACTOR * target_width +
                            (1 - INTERVAL_STABILITY_FACTOR) * interval_width)

            # Absolute bounds to prevent extremes
            interval_width = np.clip(interval_width, 1.8, 3.2)  # 1.8-3.2 unit range

            # Final price-based sanity checks
            interval_width = min(interval_width, point_prediction * 0.015)  # Max 1.5%
            interval_width = max(interval_width, point_prediction * 0.004)  # Min 0.4%

            lower_bound = point_prediction - interval_width
            upper_bound = point_prediction + interval_width

            synapse.predictions = [tao_prediction]
            synapse.intervals = [[lower_bound * 1000, upper_bound * 1000]]

            # Track performance
            response_time = time.time() - start_time
            track_prediction(
                prediction_value=point_prediction,
                actual_value=current_price,
                confidence=confidence_score,
                response_time=response_time,
                reward=0.0  # Will be updated when reward is received
            )

            
            # Log precision metrics (top miner style)
            precision_within_1pct = abs(point_prediction - current_price) / current_price <= 0.01
            logger.info(f"üéØ Precision: {'‚úì' if precision_within_1pct else '‚úó'} (‚â§1% error) | "
                       f"Interval Coverage Target: 45-50% | "
                       f"Competition Factor: {competition_factor:.2f}")
            logger.info(f"üéØ Prediction made: {tao_prediction:.2f} TAO | "
                       f"Confidence: {confidence_score:.2f} | "
                       f"Regime: {market_regime} | "
                       f"Peak: {is_peak}")

        else:
            synapse.predictions = None
            synapse.intervals = None
            logger.info(f"‚è∏Ô∏è Skipping prediction (confidence: {confidence_score:.2f} < threshold)")

    except Exception as e:
        logger.error(f"‚ùå Domination forward error: {e}")
        synapse.predictions = None
        synapse.intervals = None

    return synapse

# Initialize models on import (skip during training)
import os
if not os.getenv('TRAINING_MODE', '').lower() == 'true':
    try:
        load_domination_models()
    except Exception as e:
        logger.warning(f"Could not load domination models: {e}")
        logger.info("Running with basic prediction capabilities")
else:
    logger.info("Training mode: Skipping model loading")
